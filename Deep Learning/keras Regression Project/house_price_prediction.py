# -*- coding: utf-8 -*-
"""House price prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19WfLwHLby8C79xEpIcM1XKEfifOol0Y2

# Historical house sales data from kaggle(US)
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import files
files.upload()

df=pd.read_csv("kc_house_data.csv")

df.isnull()

df.isnull().sum()

df.describe().transpose()

plt.figure(figsize=(10,6))
sns.distplot(df["price"])

sns.countplot(df["bedrooms"])

df.corr()

df.corr()["price"].sort_values()

plt.figure(figsize=(10,5))
sns.scatterplot(x="price",y="sqft_living",data=df)

plt.figure(figsize=(10,6))
sns.scatterplot(x="price",y="long",data=df)

plt.figure(figsize=(10,6))
sns.scatterplot(x="price",y="lat",data=df)

plt.figure(figsize=(10,8))
sns.scatterplot(x="long",y="lat",data=df,hue="price")

df.sort_values("price",ascending=False).head(20)

len(df)

len(df)*0.01

non_top_1_per=df.sort_values("price",ascending=False).iloc[216:]

non_top_1_per

plt.figure(figsize=(10,8))
sns.scatterplot(x="long",y="lat",data=non_top_1_per,hue="price",edgecolor=None,alpha=0.3,palette="RdYlGn")

df=df.drop("id",axis=1)

df

df["date"]

df["date"]=pd.to_datetime(df["date"])

df["date"]

df["year"]=df["date"].apply(lambda date:date.year)
df["month"]=df["date"].apply(lambda date:date.month)

df.head()

plt.figure(figsize=(10,8))
sns.boxplot(x="month",y="price",data=df)

df.groupby("month").mean()["price"]

df.groupby("month").mean()["price"].plot()

df.groupby("year").mean()["price"].plot()

df=df.drop("zipcode",axis=1)

df.head()

df=df.drop("date",axis=1)

df

X=df.drop("price",axis=1).values
y=df["price"].values

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)

from sklearn.preprocessing import MinMaxScaler

scaler=MinMaxScaler()

X_train=scaler.fit_transform(X_train)

X_test=scaler.transform(X_test)

X_train.shape

X_test.shape

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()

model.add(Dense(19,activation='relu'))
model.add(Dense(19,activation='relu'))
model.add(Dense(19,activation='relu'))
model.add(Dense(19,activation='relu'))
model.add(Dense(1))

model.compile(optimizer='adam',loss='mse')

model.fit(x=X_train,y=y_train,
          validation_data=(X_test,y_test),
          batch_size=128,epochs=400)

model.history.history

losses=pd.DataFrame(model.history.history)

losses.head()

losses.plot()

from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score

predictions = model.predict(X_test)

mean_absolute_error(y_test,predictions)

mean_squared_error(y_test,predictions)

explained_variance_score(y_test,predictions)

mean_squared_error(y_test,predictions)**0.5

df['price'].describe()

5.402966e+05

plt.figure(figsize=(10,8))
plt.scatter(y_test,predictions)
plt.plot(y_test,y_test,"r")

single_h=df.drop("price",axis=1).iloc[0]

single_h.values.reshape(-1,19)

si=scaler.transform(single_h.values.reshape(-1,19))

model.predict(si)

df.head(1)

"""if you want our model prediction above 80%
we have to remove top 1% 
and re-train our model
"""

