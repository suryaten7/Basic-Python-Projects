# -*- coding: utf-8 -*-
"""NLP & NLTK.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BmMerdkwgqD0HyREmmLTDXivuCfXvx86
"""

import nltk

nltk.download_shell()

from google.colab import files
files.upload()

messages=[line.rstrip() for line in  open("SMSSpamCollection")]

len(messages)

messages[50]

for mess_no,message in enumerate(messages[:10]):
    print(mess_no,message)

messages[0]

import pandas as pd

messages=pd.read_csv("SMSSpamCollection",sep="\t",names=["label","message"])

messages.head()

messages.describe()

messages.groupby("label").describe()

messages["length"]=messages["message"].apply(len)

messages.head()

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

messages["length"].plot.hist(bins=200)

messages["length"].describe()

messages[messages["length"]==910]["message"].iloc[0]

messages.hist(column="length",by="label",bins=75,figsize=(12,4))

import string

mes="Sample message! notice : it has punctuation."

string.punctuation

nonpuc=[c for c in mes if c  not in string.punctuation]

nonpuc

from nltk.corpus import stopwords

stopwords.words("english")

nonpuc=''.join(nonpuc)

nonpuc

nonpuc.split()

clean_mess=[word for word in nonpuc.split() if word.lower() not in stopwords.words("english")]

clean_mess

def text_process(mess):
    nonpuc=[char for char in mess if char  not in string.punctuation]
    nonpuc=''.join(nonpuc)
    return [word for word in nonpuc.split() if word.lower() not in stopwords.words("english")]

messages["message"].head(5).apply(text_process)

from sklearn.feature_extraction.text import CountVectorizer

bow_transformer=CountVectorizer(analyzer=text_process).fit(messages["message"])

len(bow_transformer.vocabulary_)

mess4=messages["message"][3]

mess4

bow4=bow_transformer.transform([mess4])

print(bow4)

bow4.shape

bow_transformer.get_feature_names()[9554]

message_bow=bow_transformer.transform(messages["message"])

message_bow.shape

message_bow.nnz

sparsity = (100.0 * message_bow.nnz / (message_bow.shape[0] * message_bow.shape[1]))
print('sparsity: {}'.format(sparsity))

from sklearn.feature_extraction.text import TfidfTransformer

tfidf_transformer = TfidfTransformer().fit(message_bow)

tfidf4 = tfidf_transformer.transform(bow4)

print(tfidf4)

print(tfidf_transformer.idf_[bow_transformer.vocabulary_['university']])

messages_tfidf = tfidf_transformer.transform(message_bow)
print(messages_tfidf.shape)

from sklearn.naive_bayes import MultinomialNB

spam_detect_model = MultinomialNB().fit(messages_tfidf, messages['label'])

print('predicted:', spam_detect_model.predict(tfidf4)[0])
print('expected:', messages.label[3])

all_predictions = spam_detect_model.predict(messages_tfidf)
print(all_predictions)

from sklearn.metrics import classification_report
print (classification_report(messages['label'], all_predictions))

from sklearn.model_selection import train_test_split

msg_train, msg_test, label_train, label_test =train_test_split(messages['message'], messages['label'], test_size=0.3)

from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts
    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores
    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier
])

pipeline.fit(msg_train,label_train)

predictions = pipeline.predict(msg_test)

print(classification_report(label_test,predictions))

