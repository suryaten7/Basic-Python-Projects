# -*- coding: utf-8 -*-
"""KNN Example.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VEURR2q57UTjRyePAeYF5mWLA0P6XMmD
"""

import numpy as np
import pandas as pd

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

from google.colab import files

uploaded = files.upload()

df=pd.read_csv("Classified Data",index_col=0)

df.head()

from  sklearn.preprocessing import StandardScaler

scaler=StandardScaler()

scaler.fit(df.drop("TARGET CLASS",axis=1))

scaled_features=scaler.transform(df.drop("TARGET CLASS",axis=1))

scaled_features

df_feat=pd.DataFrame(scaled_features,columns=df.columns[:-1])

df_feat.head()

from sklearn.model_selection import train_test_split

X=df_feat
y=df["TARGET CLASS"]
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=101)

from sklearn.neighbors import KNeighborsClassifier

knn=KNeighborsClassifier(n_neighbors=1)

knn.fit(X_train,y_train)

pred=knn.predict(X_test)

from sklearn.metrics import classification_report,confusion_matrix

print(classification_report(y_test,pred))
print(confusion_matrix(y_test,pred))

#using elbow method to choose correct k value
error_rate=[]
for i in range(1,40):
    knn=KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train,y_train)
    pred_i=knn.predict(X_test)
    error_rate.append(np.mean(pred_i!= y_test))

plt.figure(figsize=(10,6))
plt.plot(range(1,40),error_rate,ls="--",color="red",marker="o",markerfacecolor="green",markersize=10)

#choosing k value from elbow method
knn=KNeighborsClassifier(n_neighbors=18)
knn.fit(X_train,y_train)
pred=knn.predict(X_test)
print(classification_report(y_test,pred))
print("\n")
print(confusion_matrix(y_test,pred))

#using elbow method we get good results

